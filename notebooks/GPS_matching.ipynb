{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54790c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatabase:\n",
    "    \"\"\"\n",
    "    Class describing the Custom DriveU Dataset containing a list of images\n",
    "    Attributes:\n",
    "        images (List of DriveuImage)  All images of the dataset\n",
    "        file_path (string):           Path of the dataset (.json)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, images):\n",
    "        self.images = []\n",
    "        self.input_images = images\n",
    "\n",
    "    def open(self, data_base_dir: str = \"\"):\n",
    "        \"\"\"\n",
    "        Method loading the dataset\n",
    "        Args:\n",
    "            data_base_dir(str): Base path where images are stored, optional\n",
    "            if image paths in json are outdated\n",
    "        \"\"\"\n",
    "        \n",
    "        for image_dict in self.input_images:\n",
    "            # parse and store image\n",
    "            image = DriveuImage()\n",
    "            image.parse_image_dict(image_dict, data_base_dir)\n",
    "            self.images.append(image)\n",
    "\n",
    "        return True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622b7c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from dtld_parsing.calibration import CalibrationData\n",
    "# from dtld_parsing.driveu_dataset import DriveuDatabase\n",
    "import dtld_parsing.driveu_dataset\n",
    "\n",
    "from dtld_parsing.three_dimensional_position import ThreeDimensionalPosition\n",
    "\n",
    "\n",
    "def positions_3D(args):\n",
    "    \"\"\"\n",
    "    Function from dtld_parsing repo which calculates the position of the \n",
    "    objects with respect to the camera frame in meters.\n",
    "    \"\"\"\n",
    "    # Load database\n",
    "    if args.images:\n",
    "        DriveuDatabase = CustomDatabase        \n",
    "        database = DriveuDatabase(args.images)\n",
    "    else:\n",
    "        DriveuDatabase = dtld_parsing.driveu_dataset.DriveuDatabase\n",
    "        database = DriveuDatabase(args.label_file)\n",
    "        \n",
    "    if not database.open(args.data_base_dir):\n",
    "        return False\n",
    "\n",
    "    # Load calibration\n",
    "    calibration_left = CalibrationData()\n",
    "    intrinsic_left = calibration_left.load_intrinsic_matrix(\n",
    "        args.calib_dir + \"/intrinsic_left.yml\"\n",
    "    )\n",
    "    rectification_left = calibration_left.load_rectification_matrix(\n",
    "        args.calib_dir + \"/rectification_left.yml\"\n",
    "    )\n",
    "    projection_left = calibration_left.load_projection_matrix(\n",
    "        args.calib_dir + \"/projection_left.yml\"\n",
    "    )\n",
    "    extrinsic = calibration_left.load_extrinsic_matrix(\n",
    "        args.calib_dir + \"/extrinsic.yml\"\n",
    "    )\n",
    "    distortion_left = calibration_left.load_distortion_matrix(\n",
    "        args.calib_dir + \"/distortion_left.yml\"\n",
    "    )\n",
    "    calibration_right = CalibrationData()\n",
    "    projection_right = calibration_right.load_projection_matrix(\n",
    "        args.calib_dir + \"/projection_right.yml\"\n",
    "    )\n",
    "\n",
    "    threed_position = ThreeDimensionalPosition(\n",
    "        calibration_left=calibration_left,\n",
    "        calibration_right=calibration_right,\n",
    "        binning_x=2,\n",
    "        binning_y=2,\n",
    "        roi_offset_x=0,\n",
    "        roi_offset_y=0,\n",
    "    )\n",
    "    \n",
    "    TL_coords = []\n",
    "    class_list = []\n",
    "    no_pred_id = []\n",
    "    \n",
    "    for idx_d, img in enumerate(database.images):       \n",
    "#         rects = img.map_labels_to_disparity_image(calibration_left)\n",
    "#         if img.file_path.split('/')[-2] == '2015-04-17_11-11-46':\n",
    "#             print(img.file_path)\n",
    "        try:\n",
    "            disparity_image = img.get_disparity_image()\n",
    "        except:\n",
    "            print('Reached the limit')\n",
    "            break\n",
    "        #         import pdb;pdb.set_trace()\n",
    "        \n",
    "        TL_per_img = []\n",
    "        classes_img = []\n",
    "        # Get 3D position\n",
    "        for o in img.objects:    \n",
    "            aspect = o.attributes['aspects']\n",
    "            direction = o.attributes['direction']\n",
    "            orientation = o.attributes['orientation']\n",
    "            occ = o.attributes['occlusion']\n",
    "            reflection = o.attributes['reflection']\n",
    "            x =  o.x\n",
    "            y =  o.y\n",
    "            w =  o.width\n",
    "            h =  o.height\n",
    "            if x < 0:\n",
    "                w = w + x\n",
    "                x = 0                       \n",
    "            elif y < 0:\n",
    "                h = h + y\n",
    "                y = 0\n",
    "            elif x + w > 2048:\n",
    "                w = 2048 - x\n",
    "            elif y + h > 1024:\n",
    "                h = 1024 - y \n",
    "            \n",
    "            o.x = x\n",
    "            o.y = y\n",
    "            o.width = w\n",
    "            o.height = h        \n",
    "            if aspect == '':\n",
    "                TL_per_img.append([[0, 0]])\n",
    "                continue\n",
    "            \n",
    "            if reflection == 'reflected' or occ == 'occluded' or aspect == 'four_aspects' \\\n",
    "                    or orientation == 'horizontal' or aspect == 'unknown':                \n",
    "                continue\n",
    "\n",
    "            # Camera Coordinate System: X right, Y down, Z front\n",
    "            threed_pos = threed_position.determine_three_dimensional_position(\n",
    "                o.x, o.y, o.width, o.height, disparity_image=disparity_image\n",
    "            )\n",
    "            # Get in vehicle coordinates: X front, Y left and Z up\n",
    "            threed_pos_numpy = np.array([threed_pos.get_pos()[0], \n",
    "                                         threed_pos.get_pos()[1], \n",
    "                                         threed_pos.get_pos()[2], \n",
    "                                         1])\n",
    "            threed_pos_vehicle_coordinates = extrinsic.dot(threed_pos_numpy)\n",
    "            classes = [aspect, direction]\n",
    "            TL_per_img.append(threed_pos_vehicle_coordinates[:2])   # take only x, y\n",
    "            classes_img.append(classes)\n",
    "        \n",
    "        if len(TL_per_img):   \n",
    "            TL_coords.append(np.array(TL_per_img))\n",
    "            class_list.append(classes_img)\n",
    "        else:\n",
    "            TL_coords.append(np.array([[0, 0]]))\n",
    "            class_list.append([['', '']])\n",
    "        \n",
    "    return TL_coords, class_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbc5fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import utm\n",
    "import numpy as np\n",
    "import osrm\n",
    "import folium\n",
    "import glob\n",
    "from folium import plugins    \n",
    "from math import cos, asin, sqrt, pi\n",
    "\n",
    "\n",
    "def get_GPS(json_path):\n",
    "    \"\"\"\n",
    "    From DTLD v2.0 label file, GPS measurements are taken and \n",
    "    grouped by sequences.\n",
    "    Returns grouped longitude latitude list, velocity and yaw\n",
    "    rate list, timestamp list and indices to split raw data\n",
    "    into sequence lists.\n",
    "    \"\"\"    \n",
    "    with open(json_path) as f:\n",
    "        parsed = json.load(f)    \n",
    "    data = parsed['images']\n",
    "\n",
    "    LLA = []\n",
    "    vel_list = []\n",
    "    time_list = []\n",
    "    sequence = []\n",
    "    indices = []\n",
    "    failed_ind = []\n",
    "    prev_seq = None\n",
    "\n",
    "    for idx, image in enumerate(data): \n",
    "        img_path = image['image_path']\n",
    "        path_split = img_path.split('/')   \n",
    "        curr_seq = path_split[3]\n",
    "        lat = image[\"latitude\"]\n",
    "        lon = image[\"longitude\"]\n",
    "        time_stamp = image[\"time_stamp\"]\n",
    "        vel = image[\"velocity\"]\n",
    "        yaw_rate = image[\"yaw_rate\"]\n",
    "        vel_list.append([vel, yaw_rate])\n",
    "        time_list.append(time_stamp)\n",
    "        \n",
    "        # to filter out the measurements which are not in Germany\n",
    "        # Better to save idx so that I can mask it as failed measurement\n",
    "        # while giving into KF\n",
    "        if (lat > 60.0 or lat < 40.0) or (lon > 20.0 or lon < 2.5):\n",
    "            failed_ind.append(idx)\n",
    "            continue\n",
    "            \n",
    "        if idx == 0:\n",
    "            sequence.append([lat, lon])         \n",
    "                \n",
    "#         if [lat,lon] not in sequence and curr_seq == prev_seq:\n",
    "        if curr_seq == prev_seq:\n",
    "            sequence.append([lat, lon])      \n",
    "            \n",
    "        if (curr_seq != prev_seq and idx > 0):       # checking the sequence folder\n",
    "            LLA.append(sequence)\n",
    "            indices.append(len(sequence))            \n",
    "            sequence = []\n",
    "            sequence.append([lat, lon])  # append first element of the new sequence            \n",
    "            \n",
    "        prev_seq = curr_seq\n",
    "        \n",
    "    LLA.append(sequence)   # adding last sequence to the list\n",
    "    indices = np.cumsum(indices)\n",
    "    failed_ind = np.cumsum(failed_ind)\n",
    "    \n",
    "    return LLA, vel_list, time_list, [indices, failed_ind]\n",
    "\n",
    "\n",
    "def get_UTM(LLA):\n",
    "    \"\"\"\n",
    "    Taking input as (latitude, longitude) and return UTM list as\n",
    "    (Easting, Northing, Zone Number, Zone Letter)\n",
    "    \"\"\"    \n",
    "    UTM_list = []\n",
    "    if np.shape(LLA[0]) == (2,):\n",
    "        lats = np.array(LLA).T[0]\n",
    "        longs = np.array(LLA).T[1]\n",
    "        UTM = utm.from_latlon(lats, longs)\n",
    "        UTM_list.append(UTM)\n",
    "\n",
    "    else:\n",
    "        for lla in LLA:\n",
    "            lats = np.array(lla).T[0]\n",
    "            longs = np.array(lla).T[1]\n",
    "            UTM = utm.from_latlon(lats, longs)\n",
    "            UTM_list.append(UTM)\n",
    "\n",
    "    return UTM_list\n",
    "\n",
    "\n",
    "# def calculate_distance(coord1, coord2):\n",
    "                            \n",
    "#     if len(coord1) == 2 and len(coord2) == 2:\n",
    "#         print('Calculating distance for given 2 locations')\n",
    "#         lat1, lon1 = coord1\n",
    "#         lat2, lon2 = coord2\n",
    "#     elif (np.asarray(coord1) == np.asarray(coord2)).all():\n",
    "#         lat1, lon1 = np.array(coord1[:-1]).T\n",
    "#         lat2, lon2 = np.array(coord2[1:]).T\n",
    "#     else:\n",
    "#         lat1, lon1 = np.array(coord1).T\n",
    "#         lat2, lon2 = np.array(coord2).T       \n",
    "\n",
    "#     p = pi/180\n",
    "#     ss = np.divide(np.cos(np.multiply((lat2-lat1), p)), 2) \n",
    "#     ss2 = np.multiply(np.cos(np.multiply(lat1, p)), np.cos(np.multiply(lat2, p)))\n",
    "#     ss3 = (1 - np.cos(np.multiply((lon2-lon1), p))) / 2\n",
    "#     a = 0.5 - ss + np.multiply(ss2, ss3)\n",
    "\n",
    "#     distances = 12742 * np.arcsin(np.sqrt(a)) * 1000    # 2*R*asin.. in meters\n",
    "\n",
    "#     return distances\n",
    "\n",
    "\n",
    "def match_GPS(coords, radius=20.0):\n",
    "    \"\"\"\n",
    "    Taking longitude latitude coordinates and snap them onto the\n",
    "    nearest road segment.\n",
    "    \"\"\"\n",
    "    client = osrm.Client(host='http://10.79.23.17:5000')\n",
    "    if len(coords) == 1:\n",
    "        response = client.nearest(\n",
    "            coordinates=coords,\n",
    "        #     timestamps=stamps,                \n",
    "            radiuses=[radius]*len(coords)\n",
    "        )\n",
    "        loc = response['waypoints'][0]['location']\n",
    "        locs_map = [list(reversed(loc))]\n",
    "        matchings_map = locs_map        \n",
    "    \n",
    "    else:\n",
    "        response = client.match(\n",
    "            coordinates=coords,\n",
    "        #     timestamps=stamps,\n",
    "            overview=osrm.overview.full,\n",
    "            radiuses=[radius]*len(coords)\n",
    "        )\n",
    "\n",
    "        locs = [idx['location'] for idx in response['tracepoints'] if idx]\n",
    "        locs_map = [list(reversed(loc)) for loc in locs]\n",
    "        matchings = response['matchings'][0]['geometry']['coordinates']\n",
    "        matchings_map = [list(reversed(loc)) for loc in matchings]\n",
    "   \n",
    "    return locs_map, matchings_map   \n",
    "    \n",
    "\n",
    "def mark_map(loc, grouped=False, show_popup=False, classes=False):\n",
    "    \"\"\"\n",
    "    Visualization of the positions of the traffic lights,\n",
    "    and vehicle. OSM France map is used since it has max \n",
    "    zoom 20 instead of 19. \n",
    "    'grouped' is used to visualize sequences. \n",
    "    'classes' is used for final map with class information.\n",
    "    \"\"\"\n",
    "    color_list = ['black', 'blue', 'green', 'purple', 'orange', 'darkgreen',\n",
    "                  'pink', 'lightred', 'black', 'lightgray', 'beige', 'lightblue']\n",
    "    shape_list = sorted(glob.glob('./shapes/*'))[:3] \\\n",
    "                 + sorted(glob.glob('./shapes/*'))[6:]\n",
    "    cls_list = []\n",
    "    tile_layer = folium.TileLayer(\n",
    "        tiles='https://{s}.tile.openstreetmap.fr/osmfr/{z}/{x}/{y}.png',\n",
    "        attr='&copy; OpenStreetMap France | &copy; \\\n",
    "        <a href=\"https://www.openstreetmap.org/copyright\">OpenStreetMap</a> \\\n",
    "        contributors',\n",
    "        zoom_start=18,\n",
    "        max_zoom=20,\n",
    "        name='Frmap',\n",
    "        control=False,        \n",
    "    )\n",
    "    \n",
    "    shadow = 'https://www.nicepng.com/png/full/409-4092997_lifestyle-for-mobile-shadow-png.png'\n",
    "    if classes:\n",
    "        cls_list = loc[1]\n",
    "        loc = loc[0]\n",
    "\n",
    "    if grouped:\n",
    "        max_len = max(max(len(loc_groups) for loc_groups in loc), len(loc))\n",
    "        color_list = color_list * np.ceil(max_len / len(color_list)).astype(np.int8)\n",
    "        bulb_list = ['three_aspects', 'one_aspect', 'two_aspects']\n",
    "        dir_list = ['back', 'front', 'left', 'right']\n",
    "        gt_cls_list = [list(p) for p in itertools.product(dir_list, bulb_list)]\n",
    "        \n",
    "        try:\n",
    "            # If multiple sequences with multiple images are given as input\n",
    "            my_map = folium.Map(location=loc[0][0][0], zoom_start=18, max_zoom=20, tiles=None)\n",
    "            tile_layer.add_to(my_map)  # add France map tile\n",
    "            \n",
    "            for seq_id, loc_groups in enumerate(loc):\n",
    "                # Creating feature group for each sequence to add corresponding images\n",
    "                # to the groups.\n",
    "                seq_group = folium.FeatureGroup(name=str(seq_id+1), show=show_popup)\n",
    "                my_map.add_child(seq_group)                \n",
    "                \n",
    "                for img_id ,coords in enumerate(loc_groups):\n",
    "                    # Creating feature subgroups for each image to add traffic lights\n",
    "                    # to the images.\n",
    "                    img_group = plugins.FeatureGroupSubGroup(seq_group, \n",
    "                                                             name=\"seq \"+str(seq_id+1)+\"img \"+str(img_id+1),\n",
    "                                                             show=show_popup) \n",
    "                    my_map.add_child(img_group)\n",
    "                    for idx, lla in enumerate(coords):\n",
    "#                         curr_cls = cls_list[img_id][idx]\n",
    "                        # Choosing colors for different images\n",
    "                        clr_idx = gt_cls_list.index(cls_list[img_id][idx]) if cls_list else img_id \n",
    "                        popup_name = ','.join(curr_cls) if classes else str(idx)\n",
    "                        \n",
    "                        marker = folium.Marker(location=lla, \n",
    "                                               popup=folium.Popup(popup_name, show=True),\n",
    "                                               icon=folium.map.Icon(color=color_list[clr_idx])) \n",
    "                        marker.add_to(img_group)\n",
    "                        \n",
    "#             my_map.add_child(MeasureControl()) # optional distance measure        \n",
    "            folium.LayerControl(collapsed=False).add_to(my_map)\n",
    "            \n",
    "        except: \n",
    "            # If tracked TLs with class information or multiple \n",
    "            # sequences of GPS of vehicle are given\n",
    "            max_len = max(max(len(coords) for coords in loc), len(loc))\n",
    "            color_list = color_list * np.ceil(max_len / len(color_list)).astype(np.int8)\n",
    "            my_map = folium.Map(location=loc[0][0], zoom_start=18, max_zoom=20, tiles=None)\n",
    "            tile_layer.add_to(my_map)\n",
    "            for group_id ,coords in enumerate(loc):\n",
    "                group = folium.map.FeatureGroup(name=str(group_id+1), show=show_popup)\n",
    "                for idx, lla in enumerate(coords):\n",
    "                    if classes:\n",
    "                        # If classes are given with coordinates,\n",
    "                        # custom icons are used to visualized \n",
    "                        # the traffic lights.\n",
    "                        curr_cls = cls_list[group_id][idx]\n",
    "                        curr_cls = [curr_cls[1], curr_cls[0]]                        \n",
    "                        clr_idx = gt_cls_list.index(curr_cls)\n",
    "                        popup_name = ','.join(curr_cls)\n",
    "                        # different sizes for different bulb numbers\n",
    "                        icon_sizes = [(12, 24), (12, 36), (12, 12)]\n",
    "                        icon = folium.features.CustomIcon(\n",
    "                                shape_list[clr_idx], \n",
    "                                icon_size=icon_sizes[(clr_idx+1)%3],\n",
    "                                shadow_image=shadow,\n",
    "                                shadow_size=icon_sizes[(clr_idx+1)%3],\n",
    "                                shadow_anchor=(-2, 17)\n",
    "                        )\n",
    "                        marker = folium.Marker(location=lla, \n",
    "                                               popup=folium.Popup(popup_name, show=False),\n",
    "                                               icon=icon)\n",
    "                        \n",
    "                    else:\n",
    "                        # If there is no class information, choose colors\n",
    "                        # for circular markers instead of custom icons.\n",
    "                        clr_idx = group_id\n",
    "                        popup_name = str(idx)\n",
    "                        icon = folium.map.Icon(color=color_list[clr_idx%12])\n",
    "                        marker = folium.CircleMarker(location=lla, radius=8, \n",
    "                                                     fill_color=color_list[clr_idx%12], \n",
    "                                                     color='white', fill_opacity=0.75,\n",
    "                                                     show=True)\n",
    "            \n",
    "\n",
    "                    marker.add_to(group)\n",
    "                my_map.add_child(group)\n",
    "#             my_map.add_child(MeasureControl())       \n",
    "            folium.LayerControl(collapsed=False).add_to(my_map)                 \n",
    "    \n",
    "    elif len(loc) == 2 and isinstance(loc[0], float) == 1:  \n",
    "        # only 1 coordinate is given\n",
    "        my_map = folium.Map(location=loc, zoom_start=18, max_zoom=19)\n",
    "        marker = folium.Marker(location=loc, \n",
    "                               popup=folium.Popup(str(1), show=False),\n",
    "                               )\n",
    "        marker.add_to(my_map)\n",
    "\n",
    "    else:        \n",
    "        my_map = folium.Map(location=loc[0], zoom_start=18, max_zoom=19)\n",
    "        for idx, lla in enumerate(loc):\n",
    "            marker = folium.Marker(location=lla, popup=folium.Popup(str(idx),\n",
    "                                        show=False))\n",
    "            marker.add_to(my_map)\n",
    "        \n",
    "    return my_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d3fc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "det2_smoothed_ca[7][0], TL_splitted_det2_ca[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a3290",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "video_list = [det2_smoothed_ca[7], det2_classes_split_ca[7]]\n",
    "\n",
    "mark_map(video_list, classes=True , grouped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b9203",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# video_list = [det2_smoothed_ca[0], det2_classes_split_ca[0]]\n",
    "\n",
    "# mark_map(video_list, classes=True , grouped=True)\n",
    "mark_map(smoothed_snapped[7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987c1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 7\n",
    "mark_map(smth_class, classes=True, grouped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9654766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymap3d as pm\n",
    "\n",
    "# def calculate_angle(p1, p2):\n",
    "#     \"\"\"\"\"\"\n",
    "\n",
    "#     if isinstance(p1, np.ndarray):\n",
    "#         p1_x = p1.T[0]\n",
    "#         p1_y = p1.T[1]\n",
    "#         p2_x = p2.T[0]\n",
    "#         p2_y = p2.T[1]\n",
    "#         dy = p2_y - p1_y\n",
    "#         dx = p2_x - p1_x\n",
    "#         angles = np.arctan2(dy, dx) \n",
    "        \n",
    "#         return angles\n",
    "    \n",
    "#     else:\n",
    "#         angle = np.arctan2(p2[1]-p1[1], p2[0]-p1[0])    \n",
    "      \n",
    "#         return angle\n",
    "    \n",
    "\n",
    "def to_enu(LLA):\n",
    "    \"\"\"\n",
    "    Converting longitude latitude to local ENU coordinates\n",
    "    using pymap3d package. Altitude is taken as 0 for all \n",
    "    sequences since the difference in altitude is negligible\n",
    "    for frames in a sequence.\n",
    "    \"\"\"\n",
    "    lat0 = [seq[0][0] for seq in LLA if len(seq[0])]\n",
    "    lon0 = [seq[0][1] for seq in LLA if len(seq[0])]\n",
    "    h0 = np.zeros(len(lat0))\n",
    "    lats = [np.transpose(seq)[0] for seq in LLA if len(seq[0])]\n",
    "    lons = [np.transpose(seq)[1] for seq in LLA if len(seq[0])]\n",
    "    hs = np.zeros(len(lats))\n",
    "\n",
    "    ENU = []\n",
    "    for idx in range(len(lats)):\n",
    "        e, n, u = pm.geodetic2enu(lats[idx], lons[idx], hs[idx],\n",
    "                                  lat0[idx], lon0[idx], h0[idx])\n",
    "        \n",
    "        ENU.append(np.array([e, n, u]))             \n",
    "\n",
    "    return ENU\n",
    "\n",
    "\n",
    "def angle_from_gps(gps_seq):           \n",
    "    \"\"\"\n",
    "    Calculates angle between two consecutive \n",
    "    ENU coordinates in the whole array of\n",
    "    sequence.\n",
    "    \"\"\"    \n",
    "    gps_x = gps_seq[0]\n",
    "    gps_y = gps_seq[1]\n",
    "    dy = gps_y[1:] - gps_y[:-1]\n",
    "    dx = gps_x[1:] - gps_x[:-1]\n",
    "    dy = np.append(dy, dy[-1])\n",
    "    dx = np.append(dx, dx[-1]) \n",
    "    angles = np.arctan2(dy, dx)\n",
    "    \n",
    "    return angles\n",
    "\n",
    "\n",
    "def project_TL(LLA, TL_splitted):\n",
    "    \"\"\"\n",
    "    Takes coordinate measurements of the vehicle\n",
    "    and sequence-wise grouped traffic light \n",
    "    positions with respect to the camera frame.\n",
    "    Returns projected traffic light coordinates\n",
    "    in latitude longitude, ENU, \n",
    "    \"\"\"\n",
    "    tl_latlon_list = []\n",
    "    tl_enu_list = []\n",
    "    angle_list = []\n",
    "    gps_tl_list = []\n",
    "    \n",
    "    if len(LLA[0][0]) == 2:    # [Lat, Lon] given\n",
    "        gps = to_enu(LLA)    \n",
    "        lat0 = [seq[0][0] for seq in LLA if len(seq[0])]\n",
    "        lon0 = [seq[0][1] for seq in LLA if len(seq[0])]\n",
    "        h0 = np.zeros(len(lat0))\n",
    "        hs = np.zeros(len(LLA))\n",
    "        \n",
    "    else:        \n",
    "        gps = LLA\n",
    "    \n",
    "    for seq_id, (gps_seq, tl_seq) in enumerate(zip(gps, TL_splitted)):\n",
    "        if len(tl_seq):\n",
    "            gps_arr = np.transpose(gps_seq[:2])\n",
    "            angles = angle_from_gps(gps_seq)  # using ENU coordinates         \n",
    "            R = np.array([[np.cos(angles), -np.sin(angles)],\n",
    "                          [np.sin(angles),  np.cos(angles)]])\n",
    "            R = np.transpose(R, (2,0,1))\n",
    "            \n",
    "            rotated_tl = [((R[idx] @ tl_img.T).T + gps_arr[idx]).tolist()\n",
    "                          for idx, tl_img in enumerate(tl_seq)]\n",
    "            \n",
    "            if len(gps[0]) == 4:      # UTM measurements given\n",
    "                zone_number = gps_seq[-2]\n",
    "                zone_letter = gps_seq[-1]\n",
    "                tl_latlon = [np.transpose(utm.to_latlon(tl.T[0], tl.T[1], \n",
    "                                                        zone_number, \n",
    "                                                        zone_letter)\n",
    "                                         ).tolist()\n",
    "                             for tl in rotated_tl]\n",
    "                \n",
    "            \n",
    "            else:          # ENU measurements given      \n",
    "                tl_latlon = [np.transpose(\n",
    "                             pm.enu2geodetic(np.transpose(tl)[0], np.transpose(tl)[1], hs[idx],\n",
    "                                             lat0[seq_id], lon0[seq_id], h0[seq_id]\n",
    "                                             )[:2]\n",
    "                                         ).tolist()\n",
    "                             for idx, tl in enumerate(rotated_tl)]\n",
    "                \n",
    "            tl_enu_list.append(rotated_tl)    \n",
    "            tl_latlon_list.append(tl_latlon)\n",
    "            angle_list.append(angles)\n",
    "#             gps_tl_list.append(rotated_tl)\n",
    "    \n",
    "        else:\n",
    "            tl_enu_list.append([[]])           \n",
    "            tl_latlon_list.append([[]])\n",
    "            angle_list.append([[]])\n",
    "#             gps_tl_list.append([[]])\n",
    "            \n",
    "    return tl_latlon_list, tl_enu_list, angle_list #, gps_tl_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4185145",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = argparse.ArgumentParser()\n",
    "\n",
    "# args.label_file = '/DTLD/detectron2/imsize_640_mini_p4only_4stride/DTLD_test_inference_results.json'\n",
    "# args.label_file = '/simsek/ScaledYOLOv4/runs/exp44_yolov4-csp-bulb-dir-all-multGPU-cspweights/weights/best_yolov4-csp-bulb-dir-all-multGPU-cspweights.json'\n",
    "args.calib_dir = '/dtld_parsing/calibration/'\n",
    "args.data_base_dir = '/DTLD/Berlin_disp'\n",
    "args.images = None\n",
    "# args.label_file = '/DTLD/detectron2/full_size_mini/DTLD_test_inference_results_feb.json'\n",
    "# TL_coords_det2, det2_classes = positions_3D(args)\n",
    "# args.label_file = '/DTLD/detectron2/R50_fullsize_customanchor/2022_02_23_2002_DTLD_test_inference_results_best.json'\n",
    "# args.label_file = '/DTLD/detectron2/2022_04_07_2044_default_inp_detection.json'\n",
    "# args.label_file = '/DTLD/detectron2/R50_30_130/2022_04_07_2158_DTLD_test_inference_results_best.json'\n",
    "args.label_file = '/DTLD/2022_04_11_1351_berlin1_inp_detection.json'\n",
    "TL_coords_det2_ca, det2_classes_ca = positions_3D(args)\n",
    "# args.label_file = '/DTLD/v2.0/v2.0/DTLD_test.json'\n",
    "# TL_coords_gt, gt_classes = positions_3D(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7575ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/DTLD/v2.0/v2.0/DTLD_test.json'\n",
    "LLA, vel_list, times, indices = get_GPS(path)\n",
    "LLA_rev = [[list(reversed(loc)) for loc in seq] for seq in LLA]\n",
    "snapped = []\n",
    "for idx, seq in enumerate(LLA_rev):    \n",
    "    locs_map, _ = match_GPS(seq)\n",
    "    snapped.append(locs_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70403a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snapped_utm = get_UTM(snapped)\n",
    "# TL_splitted_det2 = np.split(TL_coords_det2, indices[0])\n",
    "TL_splitted_det2_ca = np.split(TL_coords_det2_ca, indices[0])\n",
    "# TL_splitted_gt = np.split(TL_coords_gt, indices[0])\n",
    "# tl_latlon, angles, utm_tl_list = project_TL(snapped_utm, TL_splitted)\n",
    "# _, _, angles = project_TL(snapped, TL_splitted_det2_ca)\n",
    "# tl_latlon_gt, tl_enu_gt, gtangles = project_TL(snapped, TL_splitted_gt)\n",
    "# tl_smoothed_gt, _ = project_TL(smoothed_LLA, TL_splitted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c6a05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# map_groups_full_gt = [[[snap] + tl + tl_gt for snap, tl, tl_gt in zip(seq, tl_seq, tl_seq_gt) if len(tl)] \n",
    "#                        for seq, tl_seq, tl_seq_gt in zip(snapped, tl_latlon_fullsize, tl_latlon_gt)]\n",
    "# map_groups_full = [[[snap] + tl for snap, tl in zip(seq, tl_seq) if len(tl)] \n",
    "#                      for seq, tl_seq in zip(snapped, tl_latlon_fullsize)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942459d4",
   "metadata": {},
   "source": [
    "# KF for Car to improve GPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vel2velxy(gps_angle_init, vel_list, split_ids):\n",
    "    dt = 0.66\n",
    "    seq_idx = split_ids\n",
    "#     enu = to_enu(snapped)\n",
    "#     gps_angle_init = [angle_from_gps(seq)[0] for seq in enu]\n",
    "#     gps_angle_init = [seq[0] for seq in snapped]    \n",
    "    yaw_rates = np.transpose(vel_list)[1]\n",
    "    yaw_rates_splitted = np.split(yaw_rates, seq_idx)\n",
    "    theta_splitted = []\n",
    "    for seq, theta0 in zip(yaw_rates_splitted, gps_angle_init):\n",
    "        theta_seq = []\n",
    "        theta_seq.append(theta0)\n",
    "        for idx, yaw_rate in enumerate(seq):    \n",
    "            theta_seq.append(theta_seq[idx] + yaw_rate * dt)\n",
    "        theta_seq.pop(-1)\n",
    "        theta_splitted.append(theta_seq)  \n",
    "\n",
    "    velocities = np.transpose(vel_list)[0]\n",
    "    vel_splitted = np.split(velocities, seq_idx)\n",
    "    vel_xy_splitted = [vel_seq[...,None] * np.array([np.cos(theta_seq), \n",
    "                                                     np.sin(theta_seq)]).T \n",
    "                       for vel_seq, theta_seq in zip(vel_splitted, theta_splitted)]\n",
    "    \n",
    "    return [vel_xy_splitted, yaw_rates_splitted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadcd745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vel2velxy_(angles, vel_list, split_ids):\n",
    "    \"\"\"\n",
    "    Using angles calculated before to initialize\n",
    "    first angles of the sequences, and project \n",
    "    velocity of the car into East and North \n",
    "    directions by calculating the angles from\n",
    "    the yaw rate information in the dataset. \n",
    "    It also splits the resulting velocity \n",
    "    lists in sequences.\n",
    "    \"\"\"\n",
    "    dt = 0.66\n",
    "    seq_idx = split_ids\n",
    "    gps_angle_init = [seq[0] for seq in angles]\n",
    "    yaw_rates = np.transpose(vel_list)[1]\n",
    "    yaw_rates_splitted = np.split(yaw_rates, seq_idx)\n",
    "    theta_splitted = []\n",
    "    for seq, theta0 in zip(yaw_rates_splitted, gps_angle_init):\n",
    "        theta_seq = []\n",
    "        theta_seq.append(theta0)\n",
    "        \n",
    "        for idx, yaw_rate in enumerate(seq):    \n",
    "            theta_seq.append(theta_seq[idx] + yaw_rate * dt)\n",
    "            \n",
    "        theta_seq.pop(-1)\n",
    "        theta_splitted.append(theta_seq)  \n",
    "\n",
    "    velocities = np.transpose(vel_list)[0]\n",
    "    vel_splitted = np.split(velocities, seq_idx)\n",
    "    vel_xy_splitted = [vel_seq[..., None] * np.array([np.cos(theta_seq), \n",
    "                                                     np.sin(theta_seq)]).T \n",
    "                       for vel_seq, theta_seq in zip(vel_splitted, theta_splitted)]\n",
    "    \n",
    "    return [vel_xy_splitted, yaw_rates_splitted]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eba19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gps_angle_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b9aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "enu = to_enu(snapped)\n",
    "gps_angle_init = [angle_from_gps(seq)[0] for seq in enu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb0543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vel_splitted = vel2velxy(gps_angle_init, vel_list, indices[0])\n",
    "smoothed_snapped = kf_car(snapped, vel_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67686a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymap3d as pm\n",
    "def get_meas_kf(snapped, vel_splitted):\n",
    "    \"\"\"\n",
    "    Constructing measurement vector for \n",
    "    Kalman Filter which is in the form of\n",
    "    [x, x_dot, y, y_dot, theta_dot] \n",
    "    from the ENU coordinates, and splitted\n",
    "    and projected velocity data.\n",
    "    \"\"\"\n",
    "    lat0 = [seq[0][0] for seq in snapped]\n",
    "    lon0 = [seq[0][1] for seq in snapped]\n",
    "    h0 = np.zeros(len(lat0))\n",
    "    lats = [np.transpose(seq)[0] for seq in snapped]\n",
    "    lons = [np.transpose(seq)[1] for seq in snapped]\n",
    "    hs = np.zeros(len(lats))\n",
    "    xe, yn, zu = [], [], []\n",
    "\n",
    "    for idx in range(len(lats)):\n",
    "        e, n, u = pm.geodetic2enu(lats[idx], lons[idx], hs[idx],\n",
    "                                  lat0[idx], lon0[idx], h0[idx])\n",
    "        xe.append(e), yn.append(n), zu.append(u)  \n",
    "\n",
    "    # measurements = [\n",
    "    #     [[utm[0], vel_xy[0], utm[1], vel_xy[1], yaw_rate] \n",
    "    #      for utm, vel_xy, yaw_rate   \n",
    "    #      in zip(np.transpose(utm_seq[:2]), vel_xy_seq, yaw_rate_seq)] \n",
    "    #      for utm_seq, vel_xy_seq, yaw_rate_seq \n",
    "    #      in zip(snapped_utm, vel_xy_splitted, yaw_rates_splitted)\n",
    "    #     ]\n",
    "    vel_xy_splitted = vel_splitted[0]\n",
    "    yaw_rates_splitted = vel_splitted[1]    \n",
    "    measurements_enu = [\n",
    "         np.array(\n",
    "         [np.asarray([xe, vel_xy[0], yn, vel_xy[1], yaw_rate])\n",
    "         for xe, yn, vel_xy, yaw_rate   \n",
    "         in zip(xe_seq, yn_seq, vel_xy_seq, yaw_rate_seq)]\n",
    "                 ) \n",
    "         for xe_seq, yn_seq, vel_xy_seq, yaw_rate_seq \n",
    "         in zip(xe, yn, vel_xy_splitted, yaw_rates_splitted)\n",
    "        ]\n",
    "\n",
    "    meas_kf = np.array(measurements_enu)\n",
    "    return meas_kf, [lat0, lon0, h0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b957e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykalman import KalmanFilter\n",
    "import numpy as np\n",
    "\n",
    "def kf_car(snapped, vel_splitted, plot=False):\n",
    "    \"\"\"\n",
    "    Kalman Filter with Expectation-Maximization\n",
    "    algorithm for better localization of the \n",
    "    vehicle. Snapped GPS measurments, splitted \n",
    "    and projected velocity data is converted\n",
    "    KF measurements with get_meas_kf().\n",
    "    \"\"\"    \n",
    "    dt = 0.66  # calculated from the timestamps\n",
    "    \n",
    "    # states = x x_dot y y_dot theta theta_dot\n",
    "    transition_matrix = [[1, dt, 0, 0, 0, 0],\n",
    "                         [0, 1, 0, 0, 0, 0],\n",
    "                         [0, 0, 1, dt, 0, 0],\n",
    "                         [0, 0, 0, 1, 0, 0],\n",
    "                         [0, 0, 0, 0, 1, dt],\n",
    "                         [0, 0, 0, 0, 0, 1]]\n",
    "    \n",
    "    # observed states = x x_dot y y_dot theta_dot\n",
    "    observation_matrix = [[1, 0, 0, 0, 0, 0],\n",
    "                          [0, 1, 0, 0, 0, 0],\n",
    "                          [0, 0, 1, 0, 0, 0],\n",
    "                          [0, 0, 0, 1, 0, 0],                     \n",
    "                          [0, 0, 0, 0, 0, 1]]\n",
    "   \n",
    "    # get the measurements in appropriate format\n",
    "    meas_kf, lla = get_meas_kf(snapped, vel_splitted)\n",
    "    \n",
    "    # initial localization for each sequence\n",
    "    lat0, lon0, h0 = lla[0], lla[1], lla[2]    \n",
    "    smoothed_LLA = []\n",
    "    \n",
    "    for idx, meas_seq in enumerate(meas_kf):\n",
    "        if len(meas_seq):\n",
    "            \n",
    "            # Initializing states \n",
    "            initial_state_mean = [0.0,\n",
    "                                  meas_seq[0][1],\n",
    "                                  0.0,\n",
    "                                  meas_seq[0][3],\n",
    "                                  0.0,\n",
    "                                  meas_seq[0][-1]]\n",
    "            \n",
    "            kf1 = KalmanFilter(transition_matrices=transition_matrix,\n",
    "                               observation_matrices=observation_matrix,\n",
    "                               initial_state_mean=initial_state_mean,\n",
    "                               n_dim_obs=5)\n",
    "            \n",
    "            # Expectation-Maximization with 3 iterations on observation covariance\n",
    "            kf1 = kf1.em(meas_seq, \n",
    "                         n_iter=3, \n",
    "                         em_vars=['observation_covariance'])\n",
    "            \n",
    "            (smoothed_state_means, smoothed_state_covariances) = kf1.filter(meas_seq)\n",
    "                \n",
    "            # plot the smoothed states and state covariances \n",
    "            if plot:    \n",
    "\n",
    "                %matplotlib inline\n",
    "                import matplotlib                \n",
    "                import matplotlib.pyplot as plt\n",
    "                matplotlib.rcParams['figure.dpi'] = 600                \n",
    "                \n",
    "                times = range(meas_seq.shape[0])\n",
    "                fig, axs = plt.subplots(2)\n",
    "                fig.suptitle('vs Time and XY Graphs')\n",
    "                axs[0].plot(times, meas_seq[:, 0], 'bo',\n",
    "#                             times, meas_seq[:, 2], 'ro',\n",
    "#                             times, meas_seq[:, 4], 'go',         \n",
    "                            times, smoothed_state_means[:, 0], 'ro',\n",
    "#                             times, smoothed_state_means[:, 2], 'r--',\n",
    "#                             times, smoothed_state_means[:, 4], 'g--',\n",
    "                           )\n",
    "                \n",
    "                for i, txt in enumerate(meas_seq[:, 0]):\n",
    "                    axs[0].annotate(i, (times[i], txt+1), fontsize=5)\n",
    "                    axs[0].annotate(i, \n",
    "                                    (times[i], smoothed_state_means[:, 0][i]), \n",
    "                                    fontsize=5)\n",
    "                    \n",
    "                axs[1].plot(meas_seq[:, 0], meas_seq[:, 2], 'bo')\n",
    "                axs[1].plot(smoothed_state_means[:, 0], smoothed_state_means[:, 2], 'ro')\n",
    "                for i, txt in enumerate(meas_seq[:, 0]):\n",
    "#                     axs[1].annotate(round(txt), (times[i], txt+1), fontsize=5)\n",
    "                    axs[1].annotate(\n",
    "                        i, \n",
    "                        (smoothed_state_means[:, 0][i], smoothed_state_means[:, 2][i]-1.2), \n",
    "                        fontsize=5\n",
    "                    )\n",
    "                \n",
    "                plt.show()\n",
    "#                 _ = input('Press enter to continue')\n",
    "\n",
    "            lat_smooth, lon_smooth, h_smooth = pm.enu2geodetic(\n",
    "                smoothed_state_means[1:, 0], \n",
    "                smoothed_state_means[1:, 2], \n",
    "                smoothed_state_means[1:, 4],\n",
    "                lat0[idx], lon0[idx], h0[idx]\n",
    "            )        \n",
    "            lat_smooth = np.concatenate(([lat0[idx]], lat_smooth)) \n",
    "            lon_smooth = np.concatenate(([lon0[idx]], lon_smooth)) \n",
    "            smoothed_LLA.append(np.column_stack((lat_smooth, lon_smooth)).tolist())\n",
    "        else:\n",
    "            smoothed_LLA.append([[]])\n",
    "                            \n",
    "    return smoothed_LLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91290f2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# smoothed_LLA = kf_car(snapped, vel_splitted, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68fc47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLA_new, _, _, _ = get_GPS(path)\n",
    "# to_pop = [(indices[0] > idx).nonzero()[0][0] - (1 if idx > indices[0][0] else 0)\n",
    "#           for idx in aq]\n",
    "# remainder = np.array(aq) % np.array(indices[0][to_pop])\n",
    "# _ = [LLA_new[pop_id].pop(rem-1) for pop_id, rem in zip(to_pop, remainder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36d15d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothed_gt = [[[snap] + tl  for snap, tl in zip(seq, tl_seq)] \n",
    "#                  for seq, tl_seq in zip(smoothed_LLA, tl_smoothed_gt)]\n",
    "# non_smooth_gt = [[[snap] + tl  for snap, tl in zip(seq, tl_seq)] \n",
    "#                   for seq, tl_seq in zip(snapped, tl_latlon_gt)]\n",
    "# det2_smoothed_LLA = [[[snap] + tl  for snap, tl in zip(seq, tl_seq)] \n",
    "#                       for seq, tl_seq in zip(smoothed_LLA, det2_smoothed)]\n",
    "# seq_id = 0\n",
    "# mark_map(det2_smoothed_LLA[seq_id] + smoothed_gt[seq_id] + non_smooth_gt[seq_id], grouped=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980dc5a0",
   "metadata": {},
   "source": [
    "# Filtering the Traffic Lights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pykalman import KalmanFilter\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "class Track(object):\n",
    "    \"\"\"Track class for every object to be tracked.\"\"\"\n",
    "\n",
    "    def __init__(self, prediction, attributes, trackIdCount, first_seen):\n",
    "        \"\"\"\n",
    "        prediction: predicted xy of the object \n",
    "        attributes: classes from the model\n",
    "        trackIdCount: id for each track\n",
    "        first_seen: the first frame when the detection\n",
    "                    is declared as a track\n",
    "        \"\"\"\n",
    "        self.track_id = trackIdCount \n",
    "        # states are [x, y] <=> [E, N], both are measured\n",
    "        transition_matrix = [[1, 0], \n",
    "                             [0, 1]]    \n",
    "        observation_matrix = [[1, 0],\n",
    "                              [0, 1]] \n",
    "        \n",
    "        # first element of the trace of the track is initial state\n",
    "        initial_state_mean = [prediction[0],\n",
    "                              prediction[1]] \n",
    "        \n",
    "        # initialazing observation covariance matrix\n",
    "        observation_covariance = np.diag((10.0, 5.0))        \n",
    "        # initialazing state covariance matrix\n",
    "        state_covar_init = np.diag((30.0, 15.0))        \n",
    "        self.KF = KalmanFilter(transition_matrices=transition_matrix,\n",
    "                               observation_matrices=observation_matrix,\n",
    "                               initial_state_mean=initial_state_mean,\n",
    "                               observation_covariance=observation_covariance,\n",
    "                               n_dim_obs=2)  \n",
    "        # keeping last state and covariance information\n",
    "        self.last_mean = initial_state_mean\n",
    "        self.last_covar = state_covar_init        \n",
    "        self.prediction = np.asarray(prediction)  # predicted centroids (x,y)\n",
    "        self.skipped_frames = 0  # number of frames skipped undetected\n",
    "        self.trace = []  # trace path of the track\n",
    "        self.att = np.array(attributes)  # two-labeled class information\n",
    "        self.P_list = []  # keeping state covariance matrix\n",
    "        self.P_list.append(state_covar_init)\n",
    "        self.trace.append(self.prediction)  # constructing trace of the track\n",
    "        self.first_seen = first_seen\n",
    "\n",
    "class Tracker(object):\n",
    "    \"\"\"\n",
    "    Tracker class that updates track-related vectors \n",
    "    of the objects which are tracked.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dist_thresh, max_frames_to_skip):\n",
    "        \"\"\"\n",
    "        dist_thresh: distance threshold. When exceeds the threshold,\n",
    "                     track will be deleted and new track is created.\n",
    "        max_frames_to_skip: maximum allowed frames to be skipped for\n",
    "                            the track undetected.\n",
    "        min_trace_length\n",
    "        \"\"\"\n",
    "        self.dist_thresh = dist_thresh\n",
    "        self.max_frames_to_skip = max_frames_to_skip\n",
    "        self.tracks = []  \n",
    "        self.trackIdCount = 0  # trackIdCount: identification of each track\n",
    "    \n",
    "#     def group(self, detections, classes):\n",
    "#         cls = np.array(classes)\n",
    "#         try:\n",
    "#             srt = np.lexsort((cls[:,1], cls[:,0]))\n",
    "#         except:\n",
    "# #             pdb.set_trace()\n",
    "#             return detections, classes\n",
    "#         cls_srt= np.array(cls)[srt]\n",
    "#         grouped = []\n",
    "#         cls_grp = []\n",
    "#         for key, group in itertools.groupby(cls_srt, lambda x: (x[0], x[1])):\n",
    "#             group = list(group)\n",
    "#             cls_grp.append(group)\n",
    "#             grouped.append(len(group))\n",
    "\n",
    "#         group_ind = np.cumsum(grouped)\n",
    "#         det_srt = np.array(detections)[srt]    \n",
    "#         det_grp = np.split(det_srt, group_ind[:-1])\n",
    "#         det_list = []\n",
    "\n",
    "#         for idx, dets in enumerate(det_grp):\n",
    "#             if len(dets) == 1:\n",
    "#                 det_list.append(dets)\n",
    "#                 continue\n",
    "#             pwise = pdist(dets)\n",
    "#             close_dets = np.argwhere(squareform(pwise) < 5.0)\n",
    "#             couple_ind = np.argwhere(np.diff(close_dets)[:, 0] > 0)\n",
    "#             if couple_ind.size:\n",
    "#                 same_box_ind = couple_ind[:, 0] \n",
    "#                 same_dets = dets[close_dets[same_box_ind]]\n",
    "#                 same_dets = np.concatenate([] + same_dets[:].tolist(), axis=0)\n",
    "#                 mean_det = np.mean(same_dets, axis=0)\n",
    "#                 other_dets = np.delete(dets, close_dets[same_box_ind], axis=0)\n",
    "#                 cls_grp[idx] = np.delete(cls_grp[idx], close_dets[same_box_ind][:, -1], axis=0)\n",
    "#                 det_list.append(np.concatenate([mean_det[None,:], other_dets], axis=0))\n",
    "\n",
    "#             else:\n",
    "#                 det_list.append(dets)\n",
    "            \n",
    "#         detections = np.concatenate(det_list[:0] + det_list[:], axis=0)\n",
    "#         classes = np.concatenate(cls_grp[:0] + cls_grp[:], axis=0)\n",
    "            \n",
    "#         return detections, classes\n",
    "                \n",
    "    def calculate_cost(self, detections, classes):\n",
    "        \"\"\"\n",
    "        Calculating the Euclidean distance between the\n",
    "        detections and the tracks with the same class\n",
    "        information, and constructing cost matrix. \n",
    "        \"\"\"\n",
    "        N = len(self.tracks)\n",
    "        M = len(detections)\n",
    "        self.cost = np.zeros((N, M))\n",
    "        for track_id, track in enumerate(self.tracks):\n",
    "            for det_id, detection in enumerate(detections):\n",
    "                if all(track.att == classes[det_id]):\n",
    "                    # same class information \n",
    "                    diff = track.prediction - np.array(detection)\n",
    "                    # Euclidean distance\n",
    "                    distance = np.sqrt(diff.data[0]**2\n",
    "                                       + diff.data[1]**2) \n",
    "                    self.cost[track_id][det_id] = distance\n",
    "                else: \n",
    "                    # classes are not the same, put high cost\n",
    "                    self.cost[track_id][det_id] = 1e8\n",
    "                \n",
    "    def hungarian(self, detections, classes, det_idx):\n",
    "        \"\"\"\n",
    "        Deploying Hungarian Algorithm to assign the detections\n",
    "        to the correct tracks. \n",
    "        \"\"\"\n",
    "        N = len(self.tracks)\n",
    "        assignment = [-1] * N  # initializing with no matches\n",
    "        # Hungarian Algorithm\n",
    "        row_ind, col_ind = linear_sum_assignment(self.cost)\n",
    "        for i in range(len(row_ind)):\n",
    "            assignment[row_ind[i]] = col_ind[i]\n",
    "\n",
    "        # Identify tracks with no assignment, if any\n",
    "        for i in range(len(assignment)):\n",
    "            if assignment[i] != -1:\n",
    "                # check for cost distance threshold.\n",
    "                # If cost is very high then un_assign the track\n",
    "                if (self.cost[i][assignment[i]] > self.dist_thresh):\n",
    "                    assignment[i] = -1\n",
    "\n",
    "        # If tracks are not detected for a long time, remove them\n",
    "        del_tracks = []\n",
    "        for i in range(len(self.tracks)):\n",
    "            if self.tracks[i].skipped_frames >= self.max_frames_to_skip:\n",
    "                del_tracks.append(i)                \n",
    "\n",
    "        if len(del_tracks) > 0:  # only when skipped frame exceeds max\n",
    "            self.tracks = np.delete(self.tracks, del_tracks, axis=0).tolist()\n",
    "            assignment = np.delete(assignment, del_tracks, axis=0).tolist()\n",
    "        \n",
    "        # Looking for unassigned detections\n",
    "        for i in range(len(detections)):\n",
    "            if (i not in assignment):\n",
    "                # create new track for unassigned detections\n",
    "                track = Track(detections[i], \n",
    "                              classes[i],\n",
    "                              self.trackIdCount,                              \n",
    "                              det_idx)\n",
    "                self.trackIdCount +=1\n",
    "                self.tracks.append(track)        \n",
    "\n",
    "        return assignment\n",
    "\n",
    "    def update(self, detections, det_idx):\n",
    "        \"\"\"\n",
    "        - Initialize tracks\n",
    "        - Calculate cost between predicted and\n",
    "          detected centroids\n",
    "        - Assign detections to tracks using Hungarian\n",
    "          Algorithm\n",
    "        - Kalman Filter update step\n",
    "        detections: model predictions in the form of\n",
    "                    [xy, classes].        \n",
    "        det_idx: it is used as the first seen frame        \n",
    "        \"\"\"\n",
    "        classes = detections[1]\n",
    "        detections = detections[0]\n",
    "        \n",
    "        # Create tracks if no tracks vector found\n",
    "        if (len(self.tracks) == 0):          \n",
    "            for i in range(len(detections)):\n",
    "                track = Track(detections[i], classes[i], \n",
    "                              self.trackIdCount, \n",
    "                              det_idx)                \n",
    "                self.trackIdCount += 1\n",
    "                self.tracks.append(track)\n",
    "        \n",
    "        # No need for any action at first frame\n",
    "        # of the sequence.\n",
    "        if det_idx == 0:\n",
    "            return\n",
    "        \n",
    "        # calculate the cost using both xy and classes\n",
    "        self.calculate_cost(detections, classes)\n",
    "        \n",
    "        # assigning detections to tracks with Hungarian Algorithm,\n",
    "        # or creating new tracks.        \n",
    "        assignment = self.hungarian(detections, classes, det_idx)\n",
    "        \n",
    "        for i in range(len(assignment)):\n",
    "            mean_curr = self.tracks[i].last_mean\n",
    "            covar_curr = self.tracks[i].last_covar            \n",
    "            if assignment[i] != -1:\n",
    "                # since there is assignment, set skipped frames to 0\n",
    "                self.tracks[i].skipped_frames = 0\n",
    "                \n",
    "                mean_curr, covar_curr = self.tracks[i].KF.filter_update(\n",
    "                    mean_curr, \n",
    "                    covar_curr, \n",
    "                    detections[assignment[i]],\n",
    "                )\n",
    "                \n",
    "                mean_curr = mean_curr.data  # data of masked array \n",
    "            else:\n",
    "                # use covar_curr as observation covariance since there is no \n",
    "                # observation\n",
    "                mean_curr, covar_curr = self.tracks[i].KF.filter_update(\n",
    "                    mean_curr, \n",
    "                    covar_curr,\n",
    "                    observation=None,\n",
    "                    observation_covariance=covar_curr\n",
    "                )\n",
    "                # since no assignment for the track, it is skipped\n",
    "                self.tracks[i].skipped_frames += 1                \n",
    "           \n",
    "            self.tracks[i].prediction = mean_curr\n",
    "            self.tracks[i].trace.append(self.tracks[i].prediction)                \n",
    "            self.tracks[i].last_mean = mean_curr\n",
    "            self.tracks[i].last_covar = covar_curr\n",
    "            self.tracks[i].P_list.append(covar_curr)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f03494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_results_ca = tl_tracker(filtered_smoothed_ca, smoothed_LLA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af01f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def thres_filter(smoothed, heading_angles, thres):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    img_mean_list = [np.mean(img, axis=0) for seq in smoothed[0] for img in seq]\n",
    "    seq_mean_list = np.split(img_mean_list, indices[0]) #[indices[0]<151]) # 150 image for now\n",
    "    filtered_smooth = deepcopy(smoothed[0])\n",
    "    classes = deepcopy(smoothed[1])\n",
    "    if [[]] in filtered_smooth:\n",
    "        filtered_smooth = [seq for seq in filtered_smooth if seq[0]] \n",
    "        classes = [seq for seq in classes if seq.size]\n",
    "    \n",
    "    for seq_id, (seq_mean, tl_seq, angle) in enumerate(zip(seq_mean_list, filtered_smooth, heading_angles)):\n",
    "        R = np.array([[np.cos(angle), -np.sin(angle)],\n",
    "                      [np.sin(angle),  np.cos(angle)]])\n",
    "        R = np.transpose(R, (2, 0, 1))\n",
    "\n",
    "        for img_id, (img_mean, tl_img) in enumerate(zip(seq_mean, tl_seq)):            \n",
    "            diff = tl_img - img_mean\n",
    "            diff = (R[img_id] @ diff.T).T   # projecting according to heading angle of the car\n",
    "            dist2mean = np.sqrt(diff.T[0] ** 2 + 10 * diff.T[1] ** 2).T   # elliptic window \n",
    "#             if img_id == 12:\n",
    "#                 import pdb;pdb.set_trace()\n",
    "            if (dist2mean > thres).any():\n",
    "                del_id = np.nonzero(dist2mean > thres)\n",
    "                filtered_smooth[seq_id][img_id] = np.delete(filtered_smooth[seq_id][img_id], \n",
    "                                                            del_id, axis=0).tolist()\n",
    "                classes[seq_id][img_id] = np.delete(classes[seq_id][img_id], \n",
    "                                                    del_id, axis=0).tolist()\n",
    "    return [filtered_smooth, classes], seq_mean_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b8bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tl_tracker(filtered_smoothed, smoothed_LLA):\n",
    "    \"\"\"\n",
    "    filtered_smoothed: [xy, classes] of the projected \n",
    "                       traffic lights\n",
    "    smoothed_LLA: latitude longitude coordinates of the \n",
    "                  vehicle to get initial position for\n",
    "                  each sequence\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    trace_list = []\n",
    "    att_list = []\n",
    "    covar_list = []\n",
    "    first_seen_list = []\n",
    "    del_traces_list, del_covars_list, del_seen_list = [], [], []\n",
    "    \n",
    "    coords = filtered_smoothed[0]\n",
    "    classes = filtered_smoothed[1]\n",
    "    # a traffic light should be detected atleast 2 frames \n",
    "    min_trace_len = 2  \n",
    "    search_dist = 10.0  # distance threshold between the lights\n",
    "    \n",
    "    # remove empty ones due to missing disparity images\n",
    "    if [[]] in coords:\n",
    "        coords = [seq for seq in coords if seq[0]]            \n",
    "    \n",
    "    for seq_id, det_seq in enumerate(coords):\n",
    "        # set allowed skipped frames for the sequence \n",
    "        max_frames_to_skip = np.ceil(len(det_seq) / 3).astype(np.int8)\n",
    "        # initialize the Tracker for the sequence\n",
    "        tracker = Tracker(search_dist, max_frames_to_skip) \n",
    "        # get initial position of the vehicle to set it as the origin of\n",
    "        # local ENU frame\n",
    "        lat0 = smoothed_LLA[seq_id][0][0]\n",
    "        lon0 = smoothed_LLA[seq_id][0][1]\n",
    "        h0 = 0\n",
    "\n",
    "        for det_idx, det_img in enumerate(det_seq):\n",
    "            # update Tracker for each frame\n",
    "            tracker.update([det_img, \n",
    "                            classes[seq_id][det_idx]], \n",
    "                            det_idx)\n",
    "#             else:\n",
    "#                 continue\n",
    "        \n",
    "        tracks = tracker.tracks\n",
    "        traces, att, covars, first_seen, skipped \\\n",
    "            = np.transpose([[track.trace, track.att, track.P_list, \n",
    "                             track.first_seen, track.skipped_frames] \n",
    "                             for track in tracks]).tolist()\n",
    "        del_idx = []\n",
    "        for idx, trace in enumerate(traces):\n",
    "            trace = np.array(trace)\n",
    "            _, index= np.unique(trace, return_index=True,\n",
    "                                axis=0)\n",
    "            # find the frames where an assignment occurs     \n",
    "            uniq = trace[np.sort(index)]       \n",
    "            if (len(uniq) <= min_trace_len): \n",
    "                del_idx.append(idx)\n",
    "        \n",
    "        # keep deleted vectors \n",
    "        del_traces = np.array(traces)[np.array(del_idx)].tolist()\n",
    "        del_covars = np.array(covars)[np.array(del_idx)].tolist()\n",
    "        del_seen = np.array(first_seen)[np.array(del_idx)].tolist()\n",
    "        \n",
    "        traces = np.delete(traces, del_idx, axis=0).tolist()\n",
    "        att = np.delete(att, del_idx, axis=0).tolist()\n",
    "        covars = np.delete(covars, del_idx, axis=0).tolist()\n",
    "        first_seen = np.delete(first_seen, del_idx, axis=0).tolist()\n",
    "        last_coords = [group[-1] for group in traces if group]\n",
    "        if seq_id == 7:\n",
    "            import pdb;pdb.set_trace()\n",
    "        # group the last coordinates since different tracks can \n",
    "        # start from different positions but end up very close\n",
    "        # to each other\n",
    "        last_coords, att = group_last(last_coords, att)\n",
    "        # transform from ENU to latitude longitue\n",
    "        tl_latlon = [np.transpose(\n",
    "                     pm.enu2geodetic(tl.T[0], tl.T[1], h0,\n",
    "                                     lat0, lon0, h0\n",
    "                                    )[:2]\n",
    "                                 ).tolist()\n",
    "                     for idx, tl in enumerate(last_coords)] \n",
    "        \n",
    "        results.append(tl_latlon)\n",
    "        trace_list.append(traces)\n",
    "        att_list.append(att)\n",
    "        covar_list.append(covars)\n",
    "        first_seen_list.append(first_seen)\n",
    "        del_traces_list.append(del_traces)\n",
    "        del_covars_list.append(del_covars)\n",
    "        del_seen_list.append(del_seen)\n",
    "        \n",
    "    return results, trace_list, att_list, covar_list, first_seen_list, \\\n",
    "           [del_traces_list, del_covars_list, del_seen_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d22392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import fclusterdata\n",
    "\n",
    "def group_last(detections, classes):\n",
    "    \"\"\"\n",
    "    Groups the last coordinates of each traces if\n",
    "    they are closer than 5.0 m and have same class\n",
    "    information.\n",
    "    \"\"\"\n",
    "    cls = np.array(classes)\n",
    "    try:\n",
    "        srt = np.lexsort((cls[:,1], cls[:,0]))\n",
    "    except:\n",
    "        return detections, classes\n",
    "    \n",
    "    cls_srt= np.array(cls)[srt]\n",
    "    grouped = []\n",
    "    cls_grp = []\n",
    "    # grouping sorted classes\n",
    "    for key, group in itertools.groupby(cls_srt, lambda x: (x[0], x[1])):\n",
    "        group = list(group)\n",
    "        cls_grp.append(group)\n",
    "        grouped.append(len(group))\n",
    "    \n",
    "    # grouping xy of the detections according to classes\n",
    "    group_ind = np.cumsum(grouped)\n",
    "    det_srt = np.array(detections)[srt]    \n",
    "    det_grp = np.split(det_srt, group_ind[:-1])\n",
    "    det_list = []\n",
    "    \n",
    "    for idx, dets in enumerate(det_grp):\n",
    "        if len(dets) == 1:  # only one element in the group\n",
    "            det_list.append(dets[0])\n",
    "            continue\n",
    "        # sorting the xys in the group    \n",
    "#         srt = np.lexsort((dets[:,1], dets[:,0]))\n",
    "#         dets = dets[srt]  \n",
    "        # clustering the xys with 5.0m distance threshold\n",
    "        cluster_ind = fclusterdata(dets, 5.0, criterion='distance') \n",
    "#         split_ind = np.where(np.abs(np.diff(cluster_ind)) > 0)[0] + 1\n",
    "#         det_splits = np.split(dets, split_ind)\n",
    "        det_splits = [dets[cluster_ind==idx+1, :] \n",
    "                      for idx in range(max(cluster_ind ))]\n",
    "        for k, split in enumerate(det_splits):\n",
    "            if len(split) > 1:\n",
    "                del_cnt = len(split) - 1      \n",
    "                # get mean of the cluster, since they are the same object                \n",
    "                split = [np.mean(split, axis=0)]\n",
    "                # only keep one class information, delete others                \n",
    "                cls_grp[idx] = cls_grp[idx][:-del_cnt]\n",
    "            \n",
    "            det_list.append(split[0])\n",
    "\n",
    "    detections = np.concatenate([det_list], axis=0)\n",
    "    classes = np.concatenate(cls_grp[:0] + cls_grp[:], axis=0)\n",
    "\n",
    "    return detections, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# numbers = np.array([[ 76.18599767,  69.66877349],\n",
    "#        [ 75.4227973 ,  68.72071791],\n",
    "#        [ 91.81466867,  65.56888397],\n",
    "#        [ 75.51092048,  68.86807941],\n",
    "#        [106.55387751,  91.43347366],\n",
    "#        [ 92.27926526,  65.3441042 ],\n",
    "#        [ 76.38748013,  69.46072411],\n",
    "#        [ 94.31703173,  86.13376495],\n",
    "#        [126.56059911, 112.17138298],\n",
    "#        [198.47744585, 179.00238334],\n",
    "#        [132.24123637, 106.0019046 ],\n",
    "#        [159.55315234, 119.94404613],\n",
    "#        [124.50811689, 110.62058564],\n",
    "#        [162.14055616, 148.44267404],\n",
    "#        [105.88063973,  75.14234612],\n",
    "#        [123.82607071, 109.80604143],\n",
    "#        [133.97161392,  98.73633552],\n",
    "#        [126.41158668, 106.48116768]])\n",
    "\n",
    "# srt = np.lexsort((numbers[:,1], numbers[:,0]))\n",
    "# numbers = numbers[srt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2812cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vel_splitted = vel2velxy_(angles, vel_list, indices[0])\n",
    "# smoothed_LLA = kf_car(snapped, vel_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e53172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tl_smoothed_gt, smoothed_enu, _ = project_TL(smoothed_LLA, TL_splitted_gt)\n",
    "# det2_smoothed, det2_smoothed_enu, det2_angles = project_TL(smoothed_LLA, TL_splitted_det2)\n",
    "# det2_smoothed_ca, det2_smoothed_enu_ca, det2_angles_ca = project_TL(smoothed_LLA, TL_splitted_det2_ca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d793f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "det2_smoothed_ca, det2_smoothed_enu_ca, det2_angles_ca = project_TL(smoothed_snapped, TL_splitted_det2_ca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e010d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# det2_ca, det2_enu_ca, det2_ang_ca = project_TL(snapped, TL_splitted_det2_ca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be9fd6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# det2_classes_split = np.split(det2_classes, indices[0])\n",
    "# det2_smoothed_preds = [det2_smoothed_enu, det2_classes_split]\n",
    "# filtered_smoothed, seq_mean_list = thres_filter(det2_smoothed_preds, det2_angles, 150.0)\n",
    "\n",
    "det2_classes_split_ca = np.split(det2_classes_ca, indices[0])\n",
    "det2_smoothed_preds_ca = [det2_smoothed_enu_ca, det2_classes_split_ca]\n",
    "filtered_smoothed_ca, seq_mean_list_ca = thres_filter(det2_smoothed_preds_ca, det2_angles_ca, 150.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e2b22",
   "metadata": {},
   "source": [
    "# Start tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc2eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_smoothed_ca = det2_smoothed_preds_ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c042a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# track_results = tl_tracker(filtered_smoothed, smoothed_LLA)\n",
    "# res = track_results[0]\n",
    "# traces = track_results[1]\n",
    "# classes = track_results[2]\n",
    "# covar_list = track_results[3]\n",
    "# first_seen_list = track_results[4]\n",
    "# res_class = [res, classes]\n",
    "\n",
    "track_results_ca = tl_tracker(filtered_smoothed_ca, smoothed_LLA)\n",
    "# track_results_ca n= tl_tracker(filtered_smoothed_ca, snapped)\n",
    "\n",
    "# res = track_results_ca[0]\n",
    "res_smooth = track_results_ca[0]\n",
    "traces = track_results_ca[1]\n",
    "classes = track_results_ca[2]\n",
    "covar_list = track_results_ca[3]\n",
    "first_seen_list = track_results_ca[4]\n",
    "del_traces = track_results_ca[5][0]\n",
    "del_covars = track_results_ca[5][1]\n",
    "del_seen = track_results_ca[5][2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42885c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "clstr = np.array([2, 2, 1, 2, 1, 2])\n",
    "detss = np.array([[119.22331307,   6.22576122],\n",
    "                   [120.66184177,   4.10079783],\n",
    "                   [125.36069431,  19.6902085 ],\n",
    "                   [125.52098911,   4.81679856],\n",
    "                   [125.82385457,  19.63153604],\n",
    "                   [128.41290565,   5.05028084]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34588a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[detss[clstr==idx+1, :] for idx in range(max(clstr))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "smth_class = [res_smooth, classes]\n",
    "mark_map(smth_class, classes=True, grouped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ee4486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "traces[7], classes[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514391b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mark_map(det2_smoothed_ca[12] + [res_smooth[12]], grouped=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f55d14",
   "metadata": {},
   "source": [
    "# COVARIANCE ETC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7465a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq13_traces = traces[13]\n",
    "seq13_covars = covar_list[13]\n",
    "seq13_seen = first_seen_list[13]\n",
    "seq13_len = len(filtered_smoothed[0][13])\n",
    "poses = [[np.sqrt(loc.data[0]**2 + loc.data[1]**2)\n",
    "         for loc in trace] for trace in seq13_traces]\n",
    "frames = range(seq13_len)\n",
    "# poses = [pose + [pose[-1]] * (len(covar) - len(pose)) \n",
    "#          for pose, covar in zip(poses, seq13_covars)]\n",
    "len(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b1b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq13_deltraces = del_traces[13]\n",
    "seq13_delcovars = del_covars[13]\n",
    "seq13_delseen = del_seen[13]\n",
    "delposes = [[np.sqrt(loc.data[0]**2 + loc.data[1]**2)\n",
    "             for loc in trace] for trace in seq13_deltraces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c2101d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# from matplotlib.patches import Ellipse\n",
    "matplotlib.rcParams['figure.dpi'] = 600\n",
    "\n",
    "# figsize = width / float(dpi), height / float(dpi)\n",
    "#     figsize = width, height\n",
    "\n",
    "# Create a figure of the right size with one axes that takes up the full figure\n",
    "# fig = plt.figure(figsize=figsize)\n",
    "# ax = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "# ells = [\n",
    "#         [Ellipse(xy=(50-(fr+seen)*2, 50-pose/3), \n",
    "#                  width=np.diag(covar)[0], \n",
    "#                  height=np.diag(covar)[1]) \n",
    "#         for fr, pose, covar in zip(frames, pose_group, covar_group)]         \n",
    "#         for pose_group, covar_group, seen in zip(poses, \n",
    "#                                                  seq13_covars, \n",
    "#                                                  seq13_seen)\n",
    "#        ]\n",
    "\n",
    "# del_ells = [\n",
    "#             [Ellipse(xy=(50-(fr+seen)*2, 50-pose/3), \n",
    "#                      width=np.diag(covar)[0], \n",
    "#                      height=np.diag(covar)[1]) \n",
    "#             for fr, pose, covar in zip(frames, pose_group, covar_group)]         \n",
    "#             for pose_group, covar_group, seen in zip(delposes, \n",
    "#                                                      seq13_delcovars, \n",
    "#                                                      seq13_delseen)\n",
    "#            ]\n",
    "\n",
    "# plt.ion()\n",
    "# plt.rcParams.update({'axes.labelsize': 'medium'})\n",
    "# plt.rcParams.update({'xtick.labelsize': 'x-small'})\n",
    "# plt.rcParams.update({'ytick.labelsize': 'x-small'})\n",
    "\n",
    "# for e_trace, dele_trace in zip(ells, del_ells):\n",
    "#     plt.axes()\n",
    "#     plt.xlabel('Frame Number')\n",
    "#     plt.ylabel('Distance from the Car at Frame 0 (m)')\n",
    "#     plt.axis([0,25,0,150])\n",
    "#     plt.xticks(np.arange(0, 25, 1))\n",
    "#     # plt.plot(5, poses[0][0], 'ro') # against 1st x, 1st y\n",
    "#     plt.grid(True, axis='x', alpha=0.1)\n",
    "\n",
    "#     plt.twinx()\n",
    "#     plt.ylabel('Northing (m)', fontsize=10)\n",
    "#     plt.axis([0,25,50,0])\n",
    "#     plt.twiny()\n",
    "#     plt.xlabel('Easting (m)')\n",
    "#     plt.axis([50,0,50,0])\n",
    "    \n",
    "#     for idx, e in enumerate(e_trace):\n",
    "#         plt.gca().add_patch(e)\n",
    "#         e.set_alpha(0.1)\n",
    "#         plt.plot(e.center[0], e.center[1], 'ro', markersize=3) # against 1st x, 1st y\n",
    "#         try:\n",
    "#             de = dele_trace[idx]\n",
    "#             plt.gca().add_patch(de)\n",
    "#             de.set_alpha(0.1)\n",
    "#             plt.plot(de.center[0], de.center[1], 'bo', markersize=3) # against 1st x, 1st y\n",
    "#         except:\n",
    "#             continue\n",
    "    \n",
    "# #     input('stop')\n",
    "#     plt.show()\n",
    "\n",
    "covar_xy = np.array([\n",
    "            np.array([\n",
    "            np.array([fr+seen, \n",
    "             np.diag(covar)[0], \n",
    "             np.diag(covar)[1]]) \n",
    "            for fr, pose, covar in zip(frames, pose_group, covar_group)\n",
    "            ])      \n",
    "            for pose_group, covar_group, seen in zip(poses, \n",
    "                                                     seq13_covars, \n",
    "                                                     seq13_seen)\n",
    "                   ])\n",
    "\n",
    "delcovar_xy = np.array([\n",
    "            np.array([\n",
    "            np.array([fr+seen, \n",
    "             np.diag(covar)[0], \n",
    "             np.diag(covar)[1]])  \n",
    "            for fr, pose, covar in zip(frames, pose_group, covar_group)\n",
    "            ])         \n",
    "            for pose_group, covar_group, seen in zip(delposes, \n",
    "                                                     seq13_delcovars, \n",
    "                                                     seq13_delseen)\n",
    "                       ])\n",
    "\n",
    "plt.ion()\n",
    "plt.rcParams.update({'axes.labelsize': 'medium'})\n",
    "plt.rcParams.update({'xtick.labelsize': 'x-small'})\n",
    "plt.rcParams.update({'ytick.labelsize': 'x-small'})\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "size_nodet = 3\n",
    "size_det = 5\n",
    "marker_det = 'v'\n",
    "for e_trace, dele_trace in zip(covar_xy, delcovar_xy):  # CAREFUL \n",
    "    plt.axes()\n",
    "    plt.xlabel('Frame Number')\n",
    "    plt.ylabel('Distance covariance ($m^2$)')\n",
    "    plt.axis([-0.5,25,0,50])\n",
    "    plt.xticks(np.arange(0, 25, 1))\n",
    "    plt.yticks(np.arange(0, 50, 5))\n",
    "    \n",
    "    # plt.plot(5, poses[0][0], 'ro') # against 1st x, 1st y\n",
    "    plt.grid(True, axis='x', alpha=0.1)\n",
    "\n",
    "#     xnew = np.linspace(e_trace[:, 0].min(), e_trace[:, 0].max(), 150)\n",
    "#     splx = make_interp_spline(e_trace[:, 0], e_trace[:, 1], k=3)  # type: BSpline\n",
    "#     sply = make_interp_spline(e_trace[:, 0], e_trace[:, 2], k=3)\n",
    "#     covarx_smooth = splx(xnew)\n",
    "#     covary_smooth = sply(xnew)   \n",
    "#     plt.plot(xnew, covarx_smooth, 'r', linewidth=1, \n",
    "#              label='Covariance of light 1 in East direction') # against 1st x, 1st y\n",
    "#     plt.plot(xnew, covary_smooth, color=('#f4a54c'), linewidth=1, \n",
    "#              label='Covariance of light 1 in North direction') # against 1st x, 1st y\n",
    "   \n",
    "    plt.plot(e_trace[:, 0], e_trace[:, 1], 'r', linewidth=1, \n",
    "             label='Covariance of light 1 in East direction') # against 1st x, 1st y\n",
    "    plt.plot(e_trace[:, 0], e_trace[:, 2], color=('#f4a54c'), linewidth=1, \n",
    "             label='Covariance of light 1 in North direction') # against 1st x, 1st y\n",
    "    drop_idx = np.where(np.diff(e_trace[:, 1]) < 0)[0] + 1  \n",
    "    plt.plot(np.delete(e_trace[:, 0], drop_idx), np.delete(e_trace[:, 1], drop_idx), 'ro', markersize=size_nodet)  \n",
    "    plt.plot(e_trace[:, 0][drop_idx], e_trace[:, 1][drop_idx], 'r' + marker_det, markersize=size_det)\n",
    "    plt.plot(np.delete(e_trace[:, 0], drop_idx), np.delete(e_trace[:, 2], drop_idx), 'o', color=('#f4a54c'), markersize=size_nodet) # against 1st x, 1st y\n",
    "    plt.plot(e_trace[:, 0][drop_idx], e_trace[:, 2][drop_idx], marker_det, color=('#f4a54c'), markersize=size_det) # against 1st x, 1st y\n",
    "    \n",
    "#     plt.plot(e_trace[:, 0], e_trace[:, 1], 'ro', markersize=3) # against 1st x, 1st y\n",
    "#     plt.plot(e_trace[:, 0], e_trace[:, 2], 'o', color=('#f4a54c'), markersize=3) # against 1st x, 1st y    \n",
    "    \n",
    "#     del_xnew = np.linspace(dele_trace[:, 0].min(), dele_trace[:, 0].max(), 150)\n",
    "#     del_splx = make_interp_spline(dele_trace[:, 0], dele_trace[:, 1], k=1)  # type: BSpline\n",
    "#     del_sply = make_interp_spline(dele_trace[:, 0], dele_trace[:, 2], k=2)\n",
    "#     del_covarx_smooth = del_splx(del_xnew)\n",
    "#     del_covary_smooth = del_sply(del_xnew) \n",
    "    \n",
    "#     p3_x = np.poly1d(np.polyfit(dele_trace[:, 0],  dele_trace[:, 1], 9))\n",
    "#     plt.plot(del_xnew, p3_x(del_xnew), 'r', linewidth=1, label='Covariance of light 1 in East direction') # against 1st x, 1st y\n",
    "#     plt.plot(del_xnew, del_covarx_smooth, 'b', linewidth=1, label='Covariance of light 2 in East direction') # against 1st x, 1st y\n",
    "#     plt.plot(del_xnew, del_covary_smooth, color=('#4a81b8'), linewidth=1, label='Covariance of light 2 in North direction') # against 1st x, 1st y\n",
    "   \n",
    "    plt.plot(dele_trace[:, 0], dele_trace[:, 1], 'b', linewidth=1, \n",
    "             label='Covariance of light 2 in East direction') # against 1st x, 1st y\n",
    "    plt.plot(dele_trace[:, 0], dele_trace[:, 2], color=('#4a81b8'), linewidth=1, \n",
    "             label='Covariance of light 2 in North direction') # against 1st x, 1st y  \n",
    "    drop_idx = np.where(np.diff(dele_trace[:, 1]) < 0)[0] + 1\n",
    "    plt.plot(np.delete(dele_trace[:, 0], drop_idx), np.delete(dele_trace[:, 1], drop_idx), 'bo', markersize=size_nodet)    \n",
    "    plt.plot(dele_trace[:, 0][drop_idx], dele_trace[:, 1][drop_idx], 'b' + marker_det, markersize=size_det)        \n",
    "    plt.plot(np.delete(dele_trace[:, 0], drop_idx), np.delete(dele_trace[:, 2], drop_idx), 'o', color=('#4a81b8'), markersize=size_nodet) # against 1st x, 1st y\n",
    "    plt.plot(dele_trace[:, 0][drop_idx], dele_trace[:, 2][drop_idx], marker_det, color=('#4a81b8'), markersize=size_det) # against 1st x, 1st y\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.legend(fontsize='x-small')   \n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ee275",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idx = np.where(np.diff(dele_trace[:, 1]) < 0)[0] + 1\n",
    "~drop_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8a528b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib.patches import Ellipse\n",
    "matplotlib.rcParams['figure.dpi'] = 600\n",
    "import time\n",
    "\n",
    "# figsize = width / float(dpi), height / float(dpi)\n",
    "#     figsize = width, height\n",
    "\n",
    "# Create a figure of the right size with one axes that takes up the full figure\n",
    "# fig = plt.figure(figsize=figsize)\n",
    "# ax = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "ells = [\n",
    "        [Ellipse(xy=(50-(fr+seen)*2, 50-pose/2), \n",
    "                 width=np.diag(covar)[0], \n",
    "                 height=np.diag(covar)[1]) \n",
    "        for fr, pose, covar in zip(frames, pose_group, covar_group)]         \n",
    "        for pose_group, covar_group, seen in zip(poses, \n",
    "                                                 seq13_covars, \n",
    "                                                 seq13_seen)\n",
    "       ] \n",
    "\n",
    "del_ells = [\n",
    "            [Ellipse(xy=(50-(fr+seen)*2, 50-pose/3), \n",
    "                     width=np.diag(covar)[0], \n",
    "                     height=np.diag(covar)[1]) \n",
    "            for fr, pose, covar in zip(frames, pose_group, covar_group)]         \n",
    "            for pose_group, covar_group, seen in zip(delposes, \n",
    "                                                     seq13_delcovars, \n",
    "                                                     seq13_delseen)\n",
    "           ]\n",
    "\n",
    "plt.ion()\n",
    "plt.rcParams.update({'axes.labelsize': 'medium'})\n",
    "plt.rcParams.update({'xtick.labelsize': 'x-small'})\n",
    "plt.rcParams.update({'ytick.labelsize': 'x-small'})\n",
    "\n",
    "for e_trace, dele_trace in zip(ells, del_ells): # CAREFUL\n",
    "    plt.axes()\n",
    "    plt.xlabel('Frame Number')\n",
    "    plt.ylabel('Distance from the Car at Frame 0 (m)')\n",
    "    plt.axis([-0.5,25,0,100])\n",
    "    plt.xticks(np.arange(0, 25, 1))\n",
    "    # plt.plot(5, poses[0][0], 'ro') # against 1st x, 1st y\n",
    "    plt.grid(True, axis='x')\n",
    "\n",
    "#     plt.twinx()\n",
    "#     plt.ylabel('Northing (in meters)', fontsize=10)\n",
    "#     plt.axis([0,25,50,0])\n",
    "#     plt.twiny()\n",
    "#     plt.xlabel('Easting (in meters)')\n",
    "#     plt.axis([50,0,50,0])\n",
    "    \n",
    "#     for e in e_trace:\n",
    "#         plt.gca().add_patch(e)\n",
    "#         e.set_alpha(0.1)\n",
    "    for idx, e in enumerate(e_trace):\n",
    "        plt.plot((50-e.center[0])/2, (50-e.center[1])*2, 'ro', markersize=3) # against 1st x, 1st y \n",
    "        try:\n",
    "            de = dele_trace[idx]\n",
    "#             plt.gca().add_patch(de)\n",
    "#             de.set_alpha(0.1)\n",
    "            plt.plot((50-de.center[0])/2, (50-de.center[1])*2, 'bo', markersize=3) # against 1st x, 1st y\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    plt.legend(labels=['Distances of the predicted locations of traffic light 1 from the car at frame 0', \n",
    "                       'Distances of the predicted locations of traffic light 2 from the car at frame 0'], \n",
    "               fontsize='x-small',\n",
    "               loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ed61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(del_ells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab0465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tl_latlon_default, default_enu, _, _ = project_TL(LLA, TL_splitted_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64837a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_idx1_tracked = \n",
    "mark_map(res_class, grouped=True, classes=True)\n",
    "# map_idx1_tracked.save('map_idx1_tracked_last.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5da61e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx = 1\n",
    "\n",
    "map_idx1_all = mark_map([det2_smoothed[idx], det2_classes_split[idx]], grouped=True, classes=True)\n",
    "map_idx1_all.save('map_idx1_all.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14082ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/DTLD/v2.0/v2.0/DTLD_test.json', 'r') as f:\n",
    "    parsed = json.load(f)\n",
    "\n",
    "images = parsed['images']\n",
    "prev_seq = ''\n",
    "count = 0\n",
    "len_seq = 0\n",
    "for image in images:\n",
    "    path = image['image_path']\n",
    "    seq = path.split('/')[-2]\n",
    "    if seq != prev_seq:\n",
    "        print(len_seq)\n",
    "        len_seq = 1\n",
    "        count += 1\n",
    "    else:\n",
    "        len_seq += 1\n",
    "    if count == 14:\n",
    "        print(len_seq)\n",
    "        print(seq)\n",
    "        break\n",
    "    prev_seq = seq\n",
    "        \n",
    "np.diff(indices[0][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57974c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "#     if grouped:\n",
    "#         max_len = max(max(len(loc_groups) for loc_groups in loc), len(loc))\n",
    "#         color_list = color_list * np.ceil(max_len / len(color_list)).astype(np.int8)\n",
    "# #         import pdb;pdb.set_trace()\n",
    "#         try:\n",
    "# #             isinstance(loc[0][0][0], list) or isinstance(loc[0][0][0], np.ndarray):\n",
    "#             my_map = folium.Map(location=loc[0][0][0], zoom_start=18, max_zoom=19)\n",
    "#             for seq_id, loc_groups in enumerate(loc):\n",
    "#                 seq_group = folium.FeatureGroup(name=str(seq_id+1), show=show_popup)\n",
    "#                 my_map.add_child(seq_group)                \n",
    "#                 for img_id ,coords in enumerate(loc_groups):\n",
    "# #                     img_group = folium.map.FeatureGroup(name=str(img_id+1), show=show_popup)\n",
    "#                     img_group = plugins.FeatureGroupSubGroup(seq_group, \n",
    "#                                                              name=\"seq \"+str(seq_id+1)+\"img \"+str(img_id+1),\n",
    "#                                                              show=show_popup) \n",
    "#                     my_map.add_child(img_group)\n",
    "#                     for idx, lla in enumerate(coords):\n",
    "#                         popup_name = ','.join(cls_list[img_id][idx]) if classes else str(idx)                        \n",
    "#                         marker = folium.Marker(location=lla, \n",
    "#                                                popup=folium.Popup(popup_name, show=True),\n",
    "#                                                icon=folium.map.Icon(color=color_list[img_id]))        \n",
    "#                         marker.add_to(img_group)                    \n",
    "#             folium.LayerControl(collapsed=False).add_to(my_map)\n",
    "#         except:   \n",
    "#             max_len = max(max(len(coords) for coords in loc), len(loc))\n",
    "#             color_list = color_list * np.ceil(max_len / len(color_list)).astype(np.int8)\n",
    "#             my_map = folium.Map(location=loc[0][0], zoom_start=18, max_zoom=19)\n",
    "#             for group_id ,coords in enumerate(loc):\n",
    "#                 group = folium.map.FeatureGroup(name=str(group_id+1), show=show_popup)\n",
    "#                 for idx, lla in enumerate(coords):\n",
    "#                     popup_name = ','.join(cls_list[group_id][idx]) if classes else str(idx)\n",
    "#                     marker = folium.Marker(location=lla, \n",
    "#                                            popup=folium.Popup(popup_name, show=False),\n",
    "#                                            icon=folium.map.Icon(color=color_list[group_id]))        \n",
    "#                     marker.add_to(group)\n",
    "#                 my_map.add_child(group)\n",
    "#             folium.LayerControl(collapsed=False).add_to(my_map)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed1e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy\n",
    "# from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# classes = [['1', '5'], ['3', '5'], ['2', '4'], ['2', '4'], ['1', '5'], ['1', '5'], ['3', '5'], ['1', '5'], ['3', '5'], ['2', '5']]\n",
    "# srt = np.argsort(classes, axis=0)[:,0]\n",
    "# cls_srt= np.array(classes)[srt]\n",
    "# grouped = []\n",
    "# for key, group in itertools.groupby(cls_srt, lambda x: (x[0], x[1])):\n",
    "#     grouped.append(len(list(group)))\n",
    "    \n",
    "# group_ind = np.cumsum(grouped)\n",
    "# det_srt = np.array(detections)[srt]    \n",
    "# det_grp = np.split(det_srt, group_ind[:-1])\n",
    "# det_list = []\n",
    "\n",
    "# for dets in det_grp:\n",
    "#     if len(dets) == 1:\n",
    "#         det_list.append(dets)\n",
    "#         continue\n",
    "#     pwise = pdist(dets)\n",
    "#     close_dets = np.argwhere(squareform(pwise) < 15.0)\n",
    "#     couple_ind = np.argwhere(np.diff(close_dets)[:, 0]>0)\n",
    "#     if couple_ind.size:\n",
    "#         same_box_ind = couple_ind[0][0] \n",
    "#         same_dets = dets[close_dets[same_box_ind]]\n",
    "#         mean_det = np.mean(same_dets, axis=0)\n",
    "#         other_dets = np.delete(dets, close_dets[same_box_ind], axis=0)      \n",
    "#         det_list.append(np.concatenate([mean_det[None,:], other_dets], axis=0))\n",
    "#     else:\n",
    "#         det_list.append(dets) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
